### Pourquoi recourir au DQN ?

Dans notre projet, le Q-Learning tabulaire classique est inadapt√©, car il repose sur une Q-table ne pouvant g√©rer qu‚Äôun espace d‚Äô√©tats discret et de faible dimension. Or, le dataset CIC-DDoS2019 produit des √©tats continus √† haute dimension, rendant impossible toute repr√©sentation tabulaire. Nous utilisons donc le Deep Q-Learning (DQN), dans lequel la Q-table est remplac√©e par un r√©seau de neurones approximant la fonction de valeur d‚Äôaction Q(s,a). Cette fonction Q repr√©sente la qualit√© attendue d‚Äôune action dans un √©tat donn√© et permet √† l‚Äôagent d‚Äôorienter sa politique en choisissant les actions maximisant cette valeur. DQN constitue ainsi une condition n√©cessaire pour appliquer le Q-Learning dans un environnement continu et complexe comme celui de la d√©tection d‚Äôattaques DDoS.

### üìå Phase 1 ‚Äì Pr√©traitement du dataset : R√©sum√© et choix m√©thodologiques

Cette premi√®re phase avait pour objectif de transformer le dataset CIC-DDoS2019 en un format exploitable par un environnement d‚Äôapprentissage par renforcement. Elle constitue la base de tout le pipeline RL, garantissant coh√©rence, stabilit√© et reproductibilit√© des exp√©riences ult√©rieures (PPO et Q-Learning).

‚∏ª

1. Chargement et pr√©paration initiale

Les fichiers du dataset ont √©t√© t√©l√©charg√©s et charg√©s automatiquement depuis data/raw/cicddos2019/.
Plusieurs points cl√©s ont √©t√© mis en place :
	‚Ä¢	Concat√©nation multi-fichiers (training/testing, variations UDP/TCP/Benign).
	‚Ä¢	S√©lection d‚Äôun sous-ensemble pour les premiers essais (contr√¥le du volume).
	‚Ä¢	Pr√©servation de l‚Äôordre temporel lorsque disponible (important pour un mod√®le s√©quentiel RL).

Ce pr√©traitement unifi√© garantit une base coh√©rente malgr√© la structure h√©t√©rog√®ne d‚Äôorigine du dataset.

‚∏ª

2. Nettoyage des donn√©es

Un nettoyage syst√©matique a √©t√© effectu√© :
	‚Ä¢	Remplacement des valeurs inf par NaN, puis imputation √† la m√©diane (strat√©gie robuste aux distributions asym√©triques fr√©quentes en trafic r√©seau).
	‚Ä¢	Suppression des colonnes non pertinentes : identifiants, m√©tadonn√©es, colonnes constantes, timestamps inutilis√©s.
	‚Ä¢	Pr√©servation explicite de la colonne Label, m√™me lorsqu‚Äôelle est temporairement constante dans un sous-√©chantillon.

Ce nettoyage assure un dataset pleinement num√©rique et exploitable pour les m√©thodes de s√©lection et de normalisation.

‚∏ª

3. R√©duction de dimension

Deux approches √©taient envisageables : PCA ou s√©lection de features.
Apr√®s analyse m√©thodologique, nous avons retenu :

‚û§ Option choisie : S√©lection supervis√©e de features
	‚Ä¢	M√©thodes utilis√©es :
RandomForest feature importance + Mutual Information (combinaison pond√©r√©e).
	‚Ä¢	Justifications :
	‚Ä¢	Interpr√©tabilit√© plus forte que PCA.
	‚Ä¢	Stabilit√© sup√©rieure pour PPO et DQN.
	‚Ä¢	Alignement avec les pratiques en s√©curit√© r√©seau.
	‚Ä¢	Maintien du sens physique des features (ex. Flow Duration, Total Fwd Packets‚Ä¶).

Le top-k final (k = 20) constitue la base de l‚Äô√©tat dans l‚Äôenvironnement RL.

‚∏ª

4. Normalisation

Les features s√©lectionn√©es ont √©t√© normalis√©es via un StandardScaler, puis le scaler a √©t√© sauvegard√© pour garantir la reproductibilit√© des entra√Ænements.

Choix justifi√© par :
	‚Ä¢	meilleure convergence des algorithmes de type policy gradient (PPO),
	‚Ä¢	caract√©ristiques du trafic r√©seau pr√©sentant des amplitudes tr√®s diff√©rentes.

‚∏ª

5. Construction s√©quentielle (structure de l‚Äô√©tat RL)

Deux repr√©sentations possibles ont √©t√© √©tudi√©es :
	‚Ä¢	Un flux = un √©tat
	‚Ä¢	Fen√™tre glissante de flux = un √©tat

‚û§ Option retenue : fen√™tre glissante
	‚Ä¢	Param√®tre choisi : window_size = 32
	‚Ä¢	Motifs :
	‚Ä¢	capture de la dynamique temporelle d‚Äôun DDoS,
	‚Ä¢	stabilit√© accrue pour PPO,
	‚Ä¢	coh√©rence avec la litt√©rature scientifique RL + cybers√©curit√©,
	‚Ä¢	observation suffisamment riche sans √™tre trop dimensionnelle.

L‚Äô√©tat final = concat√©nation flatten√©e de 32√ó20 valeurs scal√©es.

‚∏ª

6. Sauvegarde interm√©diaire

Pour acc√©l√©rer les exp√©rimentations, le dataset final pr√©trait√© a √©t√© export√© sous :
data/processed/processed_dataset.pkl

ainsi que :
	‚Ä¢	selected_features.json
	‚Ä¢	scaler.pkl

Cette √©tape permet de relancer des entra√Ænements RL sans repasser par les √©tapes lourdes de pr√©traitement.

‚∏ª

‚úîÔ∏è Conclusion de la Phase 1

Gr√¢ce √† cette phase, nous avons obtenu :
	‚Ä¢	un dataset nettoy√©, r√©duit, normalis√©, s√©quentiel,
	‚Ä¢	une repr√©sentation d‚Äô√©tat coh√©rente pour PPO et Q-Learning,
	‚Ä¢	un pipeline reproductible, modulaire et optimis√©,
	‚Ä¢	un format final directement utilisable par l‚Äôenvironnement Gym personnalis√©.

La Phase 1 constitue ainsi un socle m√©thodologique solide pour la comparaison exp√©rimentale PPO vs Q-Learning.


Phase 4 ‚Äì Synth√®se comparative DQN vs PPO

Performances de d√©tection

Les deux agents, entra√Æn√©s et √©valu√©s selon un protocole strictement identique, atteignent une d√©tection extr√™mement efficace des attaques (recall > 99%). Toutefois, leurs comportements divergent fortement concernant la classification des flux b√©nins. PPO adopte une strat√©gie fortement biais√©e vers la pr√©diction Attack, ce qui maximise la d√©tection d‚Äôattaques mais au prix d‚Äôun taux tr√®s √©lev√© de faux positifs. √Ä l‚Äôinverse, DQN parvient √† identifier une proportion significative de trafic b√©nin tout en conservant d‚Äôexcellentes performances sur la d√©tection d‚Äôattaques.

Stabilit√© d‚Äôapprentissage

Les courbes issues de TensorBoard montrent une convergence plus r√©guli√®re pour PPO, caract√©ristique de l‚Äôapproche actor-critic. Les pertes et r√©compenses √©voluent de mani√®re plus stable. DQN, en revanche, pr√©sente des oscillations importantes tant dans la loss que dans la reward, refl√©tant la difficult√© du Q-learning dans cet espace d‚Äô√©tat compress√© et s√©quentiel.

Co√ªt de calcul

PPO est plus co√ªteux en temps et en ressources, du fait de ses multiples passes d‚Äôoptimisation et de l‚Äôentra√Ænement simultan√© d‚Äôun acteur et d‚Äôun critique. DQN, reposant sur un MLP unique et un m√©canisme de replay buffer, est plus l√©ger et plus rapide.

G√©n√©ralisation

Sur le dataset de test, DQN montre une meilleure capacit√© √† g√©n√©raliser en √©quilibrant la d√©tection du trafic l√©gitime et malveillant. PPO, bien que performant pour identifier les attaques, peine √† reconna√Ætre les flux b√©nins.

Conclusion

Dans le cadre de la d√©tection d‚Äôattaques DDoS sur CIC-DDoS2019, DQN obtient les meilleures performances globales, tandis que PPO se d√©marque par une stabilit√© d‚Äôentra√Ænement sup√©rieure. Le choix d√©pend des objectifs op√©rationnels : maximiser la d√©tection d‚Äôattaques (PPO) ou r√©duire les faux positifs en conservant un haut niveau de d√©tection (DQN).
## ğŸŸ¦ **Cadrage du projet**

### Contexte
Le projet s'inscrit dans le cadre d'une Ã©tude sur la dÃ©tection et la mitigation des attaques DDoS (Distributed Denial of Service) Ã  l'aide de techniques d'apprentissage par renforcement (RL). L'objectif est d'Ã©valuer et de comparer diffÃ©rentes stratÃ©gies RL pour protÃ©ger un rÃ©seau simulÃ© contre des attaques DDoS.

### Objectifs
- ImplÃ©menter un environnement de simulation pour les attaques DDoS.
- DÃ©velopper plusieurs agents RL capables de dÃ©tecter et de rÃ©agir aux attaques.
- Comparer les performances des agents selon des critÃ¨res dÃ©finis (taux de dÃ©tection, temps de rÃ©action, impact sur le rÃ©seau).
- Documenter les rÃ©sultats et proposer des pistes d'amÃ©lioration.

### Contraintes
- Utiliser Python et des bibliothÃ¨ques RL standards (e.g., OpenAI Gym, Stable Baselines).
- Assurer la reproductibilitÃ© des expÃ©riences.
- Respecter un cadre Ã©thique dans la simulation des attaques.

### Livrables
- Code source complet et documentÃ©.
- Rapport dÃ©taillÃ© prÃ©sentant la mÃ©thodologie, les rÃ©sultats et les analyses.
- PrÃ©sentation orale synthÃ©tisant les points clÃ©s du projet.

### Planification
1. Recherche bibliographique et dÃ©finition de l'environnement (Semaine 1-2)
2. ImplÃ©mentation des agents RL (Semaine 3-5)
3. ExpÃ©rimentations et collecte des donnÃ©es (Semaine 6-7)
4. Analyse des rÃ©sultats et rÃ©daction du rapport (Semaine 8-9)
5. PrÃ©paration de la prÃ©sentation finale (Semaine 10)

## ğŸŸ¦ Phase 1 â€” Mise en place du projet

### CrÃ©ation de lâ€™environnement Python
Un environnement virtuel a Ã©tÃ© crÃ©Ã© avec :
```
python -m venv venv
source venv/bin/activate
pip install --upgrade pip
```

### Installation des dÃ©pendances
Les dÃ©pendances suivantes ont Ã©tÃ© installÃ©es :
```
pip install numpy pandas matplotlib seaborn scikit-learn
pip install gymnasium
pip install stable-baselines3
pip install kagglehub
pip install pyarrow
```

### Structure du projet
Mise en place de lâ€™architecture standard :
src/
    agents/
    envs/
    data/
data/raw/

### TÃ©lÃ©chargement du dataset CIC-DDoS2019
Le dataset a Ã©tÃ© tÃ©lÃ©chargÃ© automatiquement grÃ¢ce au script :
python -m src.data.download_cicddos2019

### Test de lecture
Un test dans main.py a permis de confirmer la lecture dâ€™un fichier Parquet :
```
df = pd.read_parquet("data/raw/cicddos2019/UDP-training.parquet")
```

## ğŸŸ¦ Phase 2 â€” PrÃ©traitement & reprÃ©sentation des Ã©tats RL

### ğŸ¯ Objectifs
- Charger et fusionner les fichiers bruts du dataset CIC-DDoS2019.  
- Nettoyer, sÃ©lectionner et normaliser les features.  
- Structurer les donnÃ©es sous une forme exploitable pour l'apprentissage par renforcement.

### ğŸ”§ Chargement des donnÃ©es brutes
Le pipeline complet de prÃ©traitement est implÃ©mentÃ© dans :
```
src/data/preprocessing.py
```

Le chargement fusionne automatiquement tous les fichiers `.parquet` du dossier :
```
data/raw/cicddos2019/
```

Le dataset complet contient :
- **431 371 lignes**  
- **79 colonnes**

### ğŸ§½ Nettoyage des donnÃ©es
- Suppression des colonnes entiÃ¨rement vides  
- Remplacement des valeurs manquantes (`NaN`) par **0**  
- Ajout dâ€™une colonne `__source_file__` pour la traÃ§abilitÃ©  

### ğŸ§© SÃ©lection des features
Une liste de features candidates a Ã©tÃ© dÃ©finie.  
Sur celles proposÃ©es, **8** Ã©taient prÃ©sentes et utilisÃ©es :

- Flow Duration  
- Tot Fwd Pkts  
- Tot Bwd Pkts  
- TotLen Fwd Pkts  
- TotLen Bwd Pkts  
- Flow Byts/s  
- Flow Pkts/s  
- Protocol  

La cible est : **Label**

### ğŸ“ Normalisation & Split

- Standardisation via **StandardScaler()**  
- DÃ©coupage train/test : **80% / 20%**, stratifiÃ©  
- RÃ©sultats :
```
X_train : (345096, 8)
X_test  : (86275, 8)
```

### ğŸ’¾ Sauvegarde des donnÃ©es prÃ©traitÃ©es

Les objets suivants sont gÃ©nÃ©rÃ©s dans :
```
data/processed/
    X_train.npy
    X_test.npy
    y_train.npy
    y_test.npy
    scaler.pkl
```

### â–¶ï¸ ExÃ©cution du pipeline
```
python -m src.data.preprocessing
```

## ğŸŸ¦ Phase 3 â€” Formulation RL & baseline supervisÃ©e

### ğŸ¯ Objectifs
- DÃ©finir la formulation RL du problÃ¨me de dÃ©tection DDoS (MDP).
- ImplÃ©menter un environnement Gymnasium basÃ© sur les donnÃ©es prÃ©traitÃ©es.
- Mettre en place une baseline supervisÃ©e pour comparer les performances avec le RL.

### ğŸ§  Formulation RL (MDP)

- **Ã‰tats (S)** : vecteur de 8 features normalisÃ©es issu de `X_train` / `X_test`.
- **Actions (A)** :  
  - 0 = trafic normal  
  - 1 = attaque DDoS

- **RÃ©compense (R)** :  
  - +1 si lâ€™action correspond au label rÃ©el  
  - âˆ’2 pour un faux nÃ©gatif (attaque non dÃ©tectÃ©e)  
  - âˆ’1 pour un faux positif (trafic normal classÃ© comme attaque)

- **Transitions** : lâ€™agent parcourt des exemples du dataset, dans un ordre alÃ©atoire Ã  chaque Ã©pisode.

### ğŸ§© Environnement Gym â€” `DDoSDatasetEnv`

ImplÃ©mentÃ© dans :
```text
src/envs/ddos_env.py
```

### ğŸ“Š RÃ©sultats de la baseline supervisÃ©e

L'exÃ©cution de la baseline RandomForest produit automatiquement plusieurs fichiers utiles pour lâ€™analyse :

- `reports/baseline_report.md` â€” Rapport Markdown complet (rapport de classification + matrice de confusion en tableau).
- `reports/confusion_matrix.png` â€” Visualisation graphique de la matrice de confusion.
- `data/processed/baseline_random_forest.joblib` â€” ModÃ¨le entraÃ®nÃ© sauvegardÃ© pour rÃ©fÃ©rence.

Commande exÃ©cutÃ©e :
```
```bash
python -m src.agents.baseline_supervised
```

Ces Ã©lÃ©ments serviront de point de comparaison lors de la Phase 6 (expÃ©rimentations RL).

## ğŸŸ¦ Phase 4 â€” ImplÃ©mentation Q-Learning (DQN)

### ğŸ¯ Objectifs
- ImplÃ©menter une version Deep Q-Learning (DQN) adaptÃ©e aux Ã©tats continus.
- Connecter lâ€™agent DQN Ã  lâ€™environnement `DDoSDatasetEnv`.
- GÃ©nÃ©rer un premier ensemble de courbes de rÃ©compense pour comparaison ultÃ©rieure avec PPO.

### ğŸ§© Agent DQN

Lâ€™agent DQN est implÃ©mentÃ© dans :
```
src/agents/dqn_agent.py
```
CaractÃ©ristiques :
- RÃ©seau Q approximÃ© par un MLP (2 couches cachÃ©es, ReLU).
- Replay buffer (100â€¯000 transitions).
- StratÃ©gie Îµ-greedy avec dÃ©croissance linÃ©aire.
- RÃ©seau cible mis Ã  jour pÃ©riodiquement.

### â–¶ï¸ EntraÃ®nement DQN

Le script dâ€™entraÃ®nement est :
```
main_train_dqn.py
```

Commande dâ€™exemple :
```
```bash
python main_train_dqn.py --episodes 200 --device cpu
```

Les sorties sont sauvegardÃ©es dans :
```
models/dqn/
    dqn_cicddos.pt
    episode_rewards.npy
    losses.npy
```

### ğŸ“ˆ InterprÃ©tation des premiers rÃ©sultats DQN

Le reward moyen passe dâ€™environ **-760** au dÃ©but de lâ€™entraÃ®nement Ã  environ **-560** sur les Ã©pisodes les plus rÃ©cents.  
Cette amÃ©lioration montre que lâ€™agent apprend progressivement Ã  rÃ©duire ses erreurs de classification, mÃªme si un plateau apparaÃ®t aprÃ¨s une centaine dâ€™Ã©pisodes.  
Ce comportement est cohÃ©rent avec :

- une fonction de rÃ©compense fortement nÃ©gative (FN = -2, FP = -1),  
- un dataset trÃ¨s volumineux (430k flux),  
- un environnement non-Markovien (chaque flux est indÃ©pendant),  
- une phase d'exploration Îµ-greedy encore Ã©levÃ©e au dÃ©but.

Ces rÃ©sultats constituent la baseline RL initiale et seront comparÃ©s aux performances obtenues par PPO en Phase 6.

### ğŸ“ Sorties gÃ©nÃ©rÃ©es par le DQN

Les fichiers produits par l'entraÃ®nement DQN sont :

```
models/dqn/
    dqn_cicddos.pt
    episode_rewards.npy
    losses.npy
```

Ils seront utilisÃ©s lors de lâ€™analyse comparative finale (Phase 6).

## ğŸŸ¦ **Cadrage du projet**

### Contexte
Le projet s'inscrit dans le cadre d'une Ã©tude sur la dÃ©tection et la mitigation des attaques DDoS (Distributed Denial of Service) Ã  l'aide de techniques d'apprentissage par renforcement (RL). L'objectif est d'Ã©valuer et de comparer diffÃ©rentes stratÃ©gies RL pour protÃ©ger un rÃ©seau simulÃ© contre des attaques DDoS.

### Objectifs
- ImplÃ©menter un environnement de simulation pour les attaques DDoS.
- DÃ©velopper plusieurs agents RL capables de dÃ©tecter et de rÃ©agir aux attaques.
- Comparer les performances des agents selon des critÃ¨res dÃ©finis (taux de dÃ©tection, temps de rÃ©action, impact sur le rÃ©seau).
- Documenter les rÃ©sultats et proposer des pistes d'amÃ©lioration.

### Contraintes
- Utiliser Python et des bibliothÃ¨ques RL standards (e.g., OpenAI Gym, Stable Baselines).
- Assurer la reproductibilitÃ© des expÃ©riences.
- Respecter un cadre Ã©thique dans la simulation des attaques.

### Livrables
- Code source complet et documentÃ©.
- Rapport dÃ©taillÃ© prÃ©sentant la mÃ©thodologie, les rÃ©sultats et les analyses.
- PrÃ©sentation orale synthÃ©tisant les points clÃ©s du projet.

### Planification
1. Recherche bibliographique et dÃ©finition de l'environnement (Semaine 1-2)
2. ImplÃ©mentation des agents RL (Semaine 3-5)
3. ExpÃ©rimentations et collecte des donnÃ©es (Semaine 6-7)
4. Analyse des rÃ©sultats et rÃ©daction du rapport (Semaine 8-9)
5. PrÃ©paration de la prÃ©sentation finale (Semaine 10)

## ğŸŸ¦ Phase 1 â€” Mise en place du projet

### CrÃ©ation de lâ€™environnement Python
Un environnement virtuel a Ã©tÃ© crÃ©Ã© avec :
```
python -m venv venv
source venv/bin/activate
pip install --upgrade pip
```

### Installation des dÃ©pendances
Les dÃ©pendances suivantes ont Ã©tÃ© installÃ©es :
```
pip install numpy pandas matplotlib seaborn scikit-learn
pip install gymnasium
pip install stable-baselines3
pip install kagglehub
pip install pyarrow
```

### Structure du projet
Mise en place de lâ€™architecture standard :
src/
    agents/
    envs/
    data/
data/raw/

### TÃ©lÃ©chargement du dataset CIC-DDoS2019
Le dataset a Ã©tÃ© tÃ©lÃ©chargÃ© automatiquement grÃ¢ce au script :
python -m src.data.download_cicddos2019

### Test de lecture
Un test dans main.py a permis de confirmer la lecture dâ€™un fichier Parquet :
```
df = pd.read_parquet("data/raw/cicddos2019/UDP-training.parquet")
```

## ğŸŸ¦ Phase 2 â€” PrÃ©traitement & reprÃ©sentation des Ã©tats RL

### ğŸ¯ Objectifs
- Charger et fusionner les fichiers bruts du dataset CIC-DDoS2019.  
- Nettoyer, sÃ©lectionner et normaliser les features.  
- Structurer les donnÃ©es sous une forme exploitable pour l'apprentissage par renforcement.

### ğŸ”§ Chargement des donnÃ©es brutes
Le pipeline complet de prÃ©traitement est implÃ©mentÃ© dans :
```
src/data/preprocessing.py
```

Le chargement fusionne automatiquement tous les fichiers `.parquet` du dossier :
```
data/raw/cicddos2019/
```

Le dataset complet contient :
- **431 371 lignes**  
- **79 colonnes**

### ğŸ§½ Nettoyage des donnÃ©es
- Suppression des colonnes entiÃ¨rement vides  
- Remplacement des valeurs manquantes (`NaN`) par **0**  
- Ajout dâ€™une colonne `__source_file__` pour la traÃ§abilitÃ©  

### ğŸ§© SÃ©lection des features
Une liste de features candidates a Ã©tÃ© dÃ©finie.  
Sur celles proposÃ©es, **8** Ã©taient prÃ©sentes et utilisÃ©es :

- Flow Duration  
- Tot Fwd Pkts  
- Tot Bwd Pkts  
- TotLen Fwd Pkts  
- TotLen Bwd Pkts  
- Flow Byts/s  
- Flow Pkts/s  
- Protocol  

La cible est : **Label**

### ğŸ“ Normalisation & Split

- Standardisation via **StandardScaler()**  
- DÃ©coupage train/test : **80% / 20%**, stratifiÃ©  
- RÃ©sultats :
```
X_train : (345096, 8)
X_test  : (86275, 8)
```

### ğŸ’¾ Sauvegarde des donnÃ©es prÃ©traitÃ©es

Les objets suivants sont gÃ©nÃ©rÃ©s dans :
```
data/processed/
    X_train.npy
    X_test.npy
    y_train.npy
    y_test.npy
    scaler.pkl
```

### â–¶ï¸ ExÃ©cution du pipeline
```
python -m src.data.preprocessing
```

## ğŸŸ¦ Phase 3 â€” Formulation RL & baseline supervisÃ©e

### ğŸ¯ Objectifs
- DÃ©finir la formulation RL du problÃ¨me de dÃ©tection DDoS (MDP).
- ImplÃ©menter un environnement Gymnasium basÃ© sur les donnÃ©es prÃ©traitÃ©es.
- Mettre en place une baseline supervisÃ©e pour comparer les performances avec le RL.

### ğŸ§  Formulation RL (MDP)

- **Ã‰tats (S)** : vecteur de 8 features normalisÃ©es issu de `X_train` / `X_test`.
- **Actions (A)** :  
  - 0 = trafic normal  
  - 1 = attaque DDoS

- **RÃ©compense (R)** :  
  - +1 si lâ€™action correspond au label rÃ©el  
  - âˆ’2 pour un faux nÃ©gatif (attaque non dÃ©tectÃ©e)  
  - âˆ’1 pour un faux positif (trafic normal classÃ© comme attaque)

- **Transitions** : lâ€™agent parcourt des exemples du dataset, dans un ordre alÃ©atoire Ã  chaque Ã©pisode.

### ğŸ§© Environnement Gym â€” `DDoSDatasetEnv`

ImplÃ©mentÃ© dans :
```text
src/envs/ddos_env.py
```

### ğŸ“Š RÃ©sultats de la baseline supervisÃ©e

L'exÃ©cution de la baseline RandomForest produit automatiquement plusieurs fichiers utiles pour lâ€™analyse :

- `reports/baseline_report.md` â€” Rapport Markdown complet (rapport de classification + matrice de confusion en tableau).
- `reports/confusion_matrix.png` â€” Visualisation graphique de la matrice de confusion.
- `data/processed/baseline_random_forest.joblib` â€” ModÃ¨le entraÃ®nÃ© sauvegardÃ© pour rÃ©fÃ©rence.

Commande exÃ©cutÃ©e :
```
```bash
python -m src.agents.baseline_supervised
```

Ces Ã©lÃ©ments serviront de point de comparaison lors de la Phase 6 (expÃ©rimentations RL).

## ğŸŸ¦ Phase 4 â€” ImplÃ©mentation Q-Learning (DQN)

### ğŸ¯ Objectifs
- ImplÃ©menter une version Deep Q-Learning (DQN) adaptÃ©e aux Ã©tats continus.
- Connecter lâ€™agent DQN Ã  lâ€™environnement `DDoSDatasetEnv`.
- GÃ©nÃ©rer un premier ensemble de courbes de rÃ©compense pour comparaison ultÃ©rieure avec PPO.

### ğŸ§© Agent DQN

Lâ€™agent DQN est implÃ©mentÃ© dans :
```
src/agents/dqn_agent.py
```
CaractÃ©ristiques :
- RÃ©seau Q approximÃ© par un MLP (2 couches cachÃ©es, ReLU).
- Replay buffer (100â€¯000 transitions).
- StratÃ©gie Îµ-greedy avec dÃ©croissance linÃ©aire.
- RÃ©seau cible mis Ã  jour pÃ©riodiquement.

### â–¶ï¸ EntraÃ®nement DQN

Le script dâ€™entraÃ®nement est :
```
main_train_dqn.py
```

Commande dâ€™exemple :
```
```bash
python main_train_dqn.py --episodes 200 --device cpu
```

Les sorties sont sauvegardÃ©es dans :
```
models/dqn/
    dqn_cicddos.pt
    episode_rewards.npy
    losses.npy
```

### ğŸ“ˆ InterprÃ©tation des premiers rÃ©sultats DQN

Le reward moyen passe dâ€™environ **-760** au dÃ©but de lâ€™entraÃ®nement Ã  environ **-560** sur les Ã©pisodes les plus rÃ©cents.  
Cette amÃ©lioration montre que lâ€™agent apprend progressivement Ã  rÃ©duire ses erreurs de classification, mÃªme si un plateau apparaÃ®t aprÃ¨s une centaine dâ€™Ã©pisodes.  
Ce comportement est cohÃ©rent avec :

- une fonction de rÃ©compense fortement nÃ©gative (FN = -2, FP = -1),  
- un dataset trÃ¨s volumineux (430k flux),  
- un environnement non-Markovien (chaque flux est indÃ©pendant),  
- une phase d'exploration Îµ-greedy encore Ã©levÃ©e au dÃ©but.

Ces rÃ©sultats constituent la baseline RL initiale et seront comparÃ©s aux performances obtenues par PPO en Phase 6.

### ğŸ“ Sorties gÃ©nÃ©rÃ©es par le DQN

Les fichiers produits par l'entraÃ®nement DQN sont :

```
models/dqn/
    dqn_cicddos.pt
    episode_rewards.npy
    losses.npy
```

Ils seront utilisÃ©s lors de lâ€™analyse comparative finale (Phase 6).

## ğŸŸ¦ Phase 5 â€” PPO (Policy Gradient)

### ğŸ¯ Objectifs
- ImplÃ©menter un agent PPO basÃ© sur les policy gradients.
- RÃ©utiliser le mÃªme environnement `DDoSDatasetEnv` que pour DQN pour permettre une comparaison directe.
- GÃ©nÃ©rer des mÃ©triques de classification et des courbes dâ€™apprentissage comparables Ã  celles de DQN.

### ğŸ§  Rappel thÃ©orique sur PPO

PPO (Proximal Policy Optimization) est un algorithme dâ€™apprentissage par renforcement basÃ© sur les policy gradients.  
Lâ€™idÃ©e principale est de mettre Ã  jour les paramÃ¨tres de la politique \(\pi_\theta(a \mid s)\) en maximisant un objectif de type gradient de politique, tout en **limitant la taille des mises Ã  jour** pour Ã©viter les instabilitÃ©s.

Lâ€™objectif PPO utilise un ratio entre la nouvelle et lâ€™ancienne politique :

\[
r_t(\theta) = \frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_{old}}(a_t \mid s_t)}
\]

et maximise une fonction **â€œclippÃ©eâ€** :

\[
L^{CLIP}(\theta) = \mathbb{E}_t \left[ \min\left( r_t(\theta) A_t, \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) A_t \right) \right]
\]

oÃ¹ \(A_t\) est lâ€™avantage (estimÃ© typiquement via GAE) et \(\epsilon\) un hyperparamÃ¨tre de clip (ex. 0.2).  
Cette forme â€œclippÃ©eâ€ empÃªche les mises Ã  jour trop agressives qui dÃ©graderaient brutalement la politique.

Dans ce projet :
- la politique est un MLP (MlpPolicy, Stable-Baselines3),  
- lâ€™algorithme utilise PPO avec GAE, objectif clippÃ©, et optimisation par mini-batchs.

### âš™ï¸ ImplÃ©mentation PPO

Lâ€™agent PPO est implÃ©mentÃ© avec **Stable-Baselines3** :

- Fichier de configuration / helper :
  - `src/agents/ppo_agent.py` (classe `PPOConfig` et fonction `make_ppo_model(...)`).
- Script dâ€™entraÃ®nement :
  - `main_train_ppo.py`

Lâ€™entraÃ®nement est lancÃ© avec une commande du type :
```bash
python main_train_ppo.py --total-timesteps 500000 --device cpu --max-steps 1000
```

CaractÃ©ristiques :
- environnement : `DDoSDatasetEnv(split="train")` pour lâ€™entraÃ®nement, `split="test"` pour lâ€™Ã©valuation,
- politique : MlpPolicy (SB3),
- hyperparamÃ¨tres (exemple) : learning_rate = 3e-4, gamma = 0.99, clip_range = 0.2, n_steps = 2048, n_epochs = 10.

### ğŸ“Š RÃ©sultats expÃ©rimentaux PPO (premiÃ¨re passe)

Ã‰valuation sur le set de test (10 000 Ã©chantillons, labels multi-classes) :

- Accuracy globale : **22,40 %**
- Macro F1 : **0,023**
- Weighted F1 : **0,083**

Le rapport de classification montre que :
- la classe `0` (majoritaire) atteint une prÃ©cision â‰ˆ 0,23 et un recall â‰ˆ 0,999,
- toutes les autres classes (1 Ã  15) ont une prÃ©cision, un recall et un F1-score de **0,0**.

La matrice de confusion indique que :
- presque toutes les prÃ©dictions sont faites en classe `0`,
- les classes minoritaires ne sont quasiment jamais prÃ©dites.

InterprÃ©tation :
- PPO, dans cette configuration de rÃ©compense et avec des labels multi-classes fortement dÃ©sÃ©quilibrÃ©s, **collabe vers une politique triviale** : prÃ©dire systÃ©matiquement (ou quasi systÃ©matiquement) la classe majoritaire.
- Lâ€™accuracy ~22 % correspond Ã  peu prÃ¨s Ã  la proportion de la classe `0` dans le test, ce qui montre que lâ€™agent nâ€™exploite pas lâ€™information fine des autres classes.
- Pour la dÃ©tection DDoS multi-attaques, ce comportement est insuffisant : le modÃ¨le ne dÃ©tecte pas les types dâ€™attaque spÃ©cifiques (classes 1 Ã  15).

Ces rÃ©sultats motivent :
- soit une reformulation du problÃ¨me en **binaire** (benign vs attaque),
- soit un rÃ©Ã©quilibrage de la rÃ©compense (pondÃ©ration plus forte des classes dâ€™attaque),
- soit une combinaison RL + supervision pour amÃ©liorer la sensibilitÃ© aux classes minoritaires.

### ğŸ“ Sorties gÃ©nÃ©rÃ©es par PPO

Lâ€™entraÃ®nement PPO produit les fichiers suivants :

```
models/ppo/
    ppo_cicddos.zip        # modÃ¨le PPO sauvegardÃ©
    episode_rewards.npy    # rewards par Ã©pisode (Ã©valuation)
reports/
    ppo_report.md          # rapport de classification (test)
    ppo_confusion_matrix.png  # matrice de confusion PPO
```

# ğŸ“˜ **DÃ©tection dâ€™attaques DDoS par Apprentissage par Renforcement (PPO vs Q-Learning)**

---

## ğŸ”· 1. Introduction

Ce projet vise Ã  comparer lâ€™efficacitÃ© de deux algorithmes dâ€™apprentissage par renforcement (RL) â€“ **PPO (Proximal Policy Optimization)** et **Q-Learning** â€“ pour la dÃ©tection dâ€™attaques DDoS dans un environnement simulÃ© de rÃ©seau.

---

## ğŸ”· 2. Objectifs

- Concevoir un environnement simulant des attaques DDoS.
- ImplÃ©menter et entraÃ®ner des agents RL avec PPO et Q-Learning.
- Comparer leurs performances en termes de dÃ©tection, prÃ©cision et temps dâ€™apprentissage.

---

## ğŸ”· 3. PrÃ©requis

- Python 3.8+
- pip

---

## ğŸ”· Dataset CIC-DDoS2019

Le projet utilise le dataset CIC-DDoS2019 (Canadian Institute for Cybersecurity).
Le tÃ©lÃ©chargement se fait automatiquement via kagglehub :

python -m src.data.download_cicddos2019

---

## ğŸ”· 4. Installation

1. **Cloner le dÃ©pÃ´t :**
   ```bash
   git clone https://github.com/votre-utilisateur/ddos_rl_benchmark.git
   cd ddos_rl_benchmark
   ```
2. **CrÃ©er un environnement virtuel (optionnel mais recommandÃ©) :**
   ```bash
   python -m venv venv
   source venv/bin/activate  # Sur Windows: venv\Scripts\activate
   ```
3. **Installer les dÃ©pendances :**
   ```bash
   pip install -r requirements.txt
   ```
4. **TÃ©lÃ©charger le dataset :**
   ```bash
   python -m src.data.download_cicddos2019
   ```

---

## ğŸ”· 5. Structure du projet

```
ddos_rl_benchmark/
â”‚â”€â”€ data/
â”‚   â””â”€â”€ raw/
â”‚â”€â”€ src/
â”‚   â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ envs/
â”‚   â””â”€â”€ data/
â”‚â”€â”€ main.py
â”‚â”€â”€ notes.md
â”‚â”€â”€ README.md
â”‚â”€â”€ requirements.txt
```

*Note : Avant dâ€™entraÃ®ner les agents, il est nÃ©cessaire de prÃ©traiter les donnÃ©es.*

### ğŸ”· Phase 2 â€” PrÃ©traitement

Avant lâ€™entraÃ®nement des agents, lancer le pipeline de prÃ©traitement :
```
python -m src.data.preprocessing
```
Cela gÃ©nÃ¨re automatiquement les fichiers normalisÃ©s dans `data/processed/`.

### ğŸ”· Phase 3 â€” Baseline supervisÃ©e

Pour entraÃ®ner la baseline supervisÃ©e (RandomForest) :

```bash
python -m src.agents.baseline_supervised
```

Ce script gÃ©nÃ¨re automatiquement :

- `reports/baseline_report.md` â€” rapport lisible en Markdown
- `reports/confusion_matrix.png` â€” heatmap de la matrice de confusion
- `data/processed/baseline_random_forest.joblib` â€” modÃ¨le sauvegardÃ©

Lâ€™environnement RL utilisÃ© par les futurs agents (Q-Learning et PPO) est dÃ©fini dans :

```
src/envs/ddos_env.py
```

---

### ğŸ”· Phase 4 â€” Q-Learning (DQN)

EntraÃ®ner lâ€™agent DQN (version Deep Q-Learning) sur lâ€™environnement DDoS :

Exemple dâ€™entraÃ®nement :
```bash
python main_train_dqn.py --episodes 200 --device cpu --split train --out-dir models/dqn
```

Les modÃ¨les et courbes dâ€™entraÃ®nement sont sauvegardÃ©s dans :

```
models/dqn/
```

*Note : PPO sera ajoutÃ© en Phase 5. Les commandes PPO dans la section â€œUtilisationâ€ seront activÃ©es une fois cette phase complÃ©tÃ©e.*

---

### ğŸ”· Phase 5 â€” PPO (Policy Gradient)

EntraÃ®ner lâ€™agent PPO (Stable-Baselines3) sur le mÃªme environnement DDoS :

```bash
python main_train_ppo.py --total-timesteps 500000 --device cpu --max-steps 1000
```

Les modÃ¨les et rapports gÃ©nÃ©rÃ©s sont sauvegardÃ©s dans :

```
models/ppo/
reports/
```

- `models/ppo/ppo_cicddos.zip` : modÃ¨le PPO entraÃ®nÃ©
- `reports/ppo_report.md` : rapport de classification sur le test
- `reports/ppo_confusion_matrix.png` : matrice de confusion PPO
```

---

## ğŸ”· 6. Utilisation

âš ï¸ *Cette section sera mise Ã  jour lorsque les scripts dâ€™entraÃ®nement
(PPO et Q-Learning) seront finalisÃ©s.*

Les commandes ci-dessous sont indicatives et seront ajustÃ©es :

### Lancer une expÃ©rience PPO :
```bash
python main.py --algo ppo --episodes 1000
```

### Lancer une expÃ©rience Q-Learning :
```bash
python main.py --algo qlearning --episodes 1000
```

### Options principales :
- `--algo` : Choix de lâ€™algorithme (`ppo` ou `qlearning`)
- `--episodes` : Nombre dâ€™Ã©pisodes dâ€™entraÃ®nement
- `--render` : Affiche lâ€™environnement (si applicable)

---

## ğŸ”· 7. RÃ©sultats attendus

- **Courbes dâ€™apprentissage** : PrÃ©cision, taux de dÃ©tection, taux de faux positifs.
- **Comparaison** : Tableaux comparatifs entre PPO et Q-Learning.
- **ReproductibilitÃ©** : Scripts et seeds pour rÃ©pÃ©ter les expÃ©riences.

---

## ğŸ”· 8. RÃ©fÃ©rences

- [OpenAI Gym](https://gym.openai.com/)
- [Stable Baselines3](https://stable-baselines3.readthedocs.io/)
- [Introduction to Reinforcement Learning (Sutton & Barto)](http://incompleteideas.net/book/the-book.html)

---

## ğŸ”· 9. Auteurs

- **Nathan HÃ©rault** â€“ UQO
- **BafodÃ© Koulibaly** â€“ UQO

---

## ğŸ”· 10. Licence

Ce projet est sous licence MIT.